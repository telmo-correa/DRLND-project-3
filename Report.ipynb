{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will see an implementation for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation details\n",
    "\n",
    "### Summary\n",
    "\n",
    "The problem consists of implementing mostly-cooperative agents to play a game of Tennis.  \n",
    "- 2 agents control rackets to bounce a ball over a net\n",
    "- If an agent hits a ball over the net, it receives a reward of +0.1\n",
    "- If an agent lets a ball hit the ground or go out of bounds, it receives a reward of -0.01.\n",
    "- Episodes last until a ball hits the ground, or at most 5000 steps.\n",
    "- The observation space consists of 8 variables for each agent.\n",
    "- The action space consists of 2 continuous variables for each agent.\n",
    "\n",
    "The problem is considered solved when the maximum score over both agents, averaged over 100 consecutive episodes, is at least 0.5.  However, we are going to go above and beyond and train the problem to a solved score of **1.5** instead.\n",
    "\n",
    "We implement Multi-Agent Deep Deterministic Policy Gradient (MAADPG), presented in the paper [Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/abs/1706.02275).  This is a variant of the DDPG algorithm; each agent consists of a actor network and a critic network.  \n",
    "\n",
    "Each actor network observes only the states for that agent, producing an action; each critic network observes (during training only) all states and all actions, producing a reward estimate for that agent.\n",
    "\n",
    "Experiences for all agents and states are logged on a shared replay buffer, which is sampled at every number of steps.\n",
    "\n",
    "### Networks\n",
    "\n",
    "Two actor networks and two critic networks are maintained, during training, for each agent:\n",
    "- Local actor network\n",
    "- Target actor network\n",
    "- Local critic network\n",
    "- Target critic network\n",
    "\n",
    "Updates are performed to the local network, and then to the target network, in the same manner as in single-agent DDPG.  During execution, only the local actor network is used.\n",
    "\n",
    "#### Actor\n",
    "\n",
    "Actor networks deterministically generate the policy that are used at execution time, taking into input only the state observable by that agent.\n",
    "\n",
    "$$ \\pi_1(s_1) = (a_{11}, a_{12}) $$\n",
    "$$ \\pi_2(s_2) = (a_{21}, a_{22}) $$\n",
    "\n",
    "During training, a Ornstein–Uhlenbeck noise process is added to this generated policy, which is then clipped between -1 and 1.  The resulting policies, used during training, are:\n",
    "\n",
    "$$ \\tilde{\\pi}_1(s_1) = \\mathrm{clip} (\\pi_1(s_1) + \\epsilon_1(t), -1, 1) $$\n",
    "$$ \\tilde{\\pi}_2(s_2) = \\mathrm{clip} (\\pi_2(s_2) + \\epsilon_2(t), -1, 1) $$\n",
    "\n",
    "The actor network for each agent is implemented as a two-layer fully connected neural network with batch normalization at each step, and a tanh operation at the end:\n",
    "\n",
    "      output = nn.Sequential(\n",
    "          nn.BatchNorm1d(state_size),\n",
    "          nn.Linear(state_size, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.BatchNorm1d(64),\n",
    "          nn.Linear(64, 64),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(64, action_size),\n",
    "          nn.Tanh()\n",
    "      )(state)\n",
    "      \n",
    "The actor network is implemented in `model.py`.\n",
    "\n",
    "#### Critic\n",
    "\n",
    "Critic networks deterministically generated a Q-value used during training.  It takes into input the states observed by each agent, as well as the inputs generated for each agent:\n",
    "\n",
    "$$ Q_1(s_1, s_2, \\tilde{\\pi}_1(s_1), \\tilde{\\pi}_2(s_2)) = v_1 $$\n",
    "$$ Q_2(s_1, s_2, \\tilde{\\pi}_1(s_1), \\tilde{\\pi}_2(s_2)) = v_2 $$\n",
    "\n",
    "The critic network applies a batch normalization to the state inputs, concatenates them with the action inputs, and then passes them through a fully connected network:\n",
    "\n",
    "    normalized_states = nn.BatchNorm1d(state_size * num_agents)(states)\n",
    "    concatenated_input = torch.cat((normalized_states, actions), dim=1)\n",
    "    output = nn.Sequential(\n",
    "        nn.Linear((state_size + action_size) * num_agents, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 64),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(64, 1)\n",
    "    )(concatenated_input)\n",
    "    \n",
    "The critic network is implemented in `model.py`.\n",
    "\n",
    "#### Noise process\n",
    "\n",
    "The noise process used during training is a stateful Ornstein-Uhlenbeck process associated with each agent and action size, with drift $\\mu = 0$:\n",
    "\n",
    "    x := state\n",
    "    dx := theta * (mu - x) + sigma * gaussian_noise\n",
    "    state := x + dx\n",
    "    return state * scale\n",
    "\n",
    "The noise process is implemented in `noise.py`.\n",
    "\n",
    "### Training\n",
    "\n",
    "#### Replay buffer\n",
    "\n",
    "We use a shared replay buffer, implemented in `memory.py`, to keep track of the observed experiences for all agents at each step:\n",
    "\n",
    "- **state**: Tuple of states ($s_1$, $s_2$) observed by each agent\n",
    "- **action**: Tuple of actions ($\\tilde{\\pi}_1(s_1)$, $\\tilde{\\pi}_2(s_2)$) selected by each agent\n",
    "- **reward**: Tuple of reward values ($r_1$, $r_2$) provided to each agent\n",
    "- **next_state**: Tuple of future states ($s'_1$, $s'_2$) reached by each agent after taking the actions\n",
    "- **done**: Tuple of flags determining whether the agents reached a terminal state\n",
    "\n",
    "Experiences are stored on a rolling buffer.  At each training step, a batch of random experiences (with replacement) is selected, and used on the optimization steps.\n",
    "\n",
    "#### Loss functions\n",
    "\n",
    "The critic loss for each agent is computed as the mean-square error between the target rewards and the expected rewards:\n",
    "    \n",
    "    for each agent:\n",
    "        target_actions := [pi_i(next_state[i]) for i in agents]\n",
    "        Q_targets_next := agent.target_critic(next_state, target_actions)\n",
    "        Q_targets := rewards[agent] + gamma * Q_targets_next * (1 - done[agent])\n",
    "        Q_expected := agent.critic(state, action)\n",
    "        critic_loss := MSE(Q_targets, Q_expected)\n",
    "        \n",
    "The actor loss for each agent is computed based on the critic value:\n",
    "\n",
    "    for each agent:\n",
    "        Q_input := [pi_i(state[i]) for i in agents]\n",
    "        actor_loss := -agent.critic(state, Q_input).mean()\n",
    "        \n",
    "Independent optimizers are used for the action and critic losses, with clipped gradient values.\n",
    "\n",
    "#### Soft target updates\n",
    "\n",
    "After each optimization step, a soft target update is performed on the actor and critic networks:\n",
    "\n",
    "    target_params := target_params * (1 - tau) + local_params * tau\n",
    "\n",
    "The training process is implemented in `ddpg.py` and `maddpg.py`.\n",
    "\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Agent hyperparameters may be passed as constructor arguments to `MADDPG`.  The default values, used in this workbook, are:\n",
    "\n",
    "| parameter                      | value      | description                                                             |\n",
    "|--------------------------------|------------|-------------------------------------------------------------------------|     \n",
    "| actor_network_units            | (64, 64)   | Network topology for actor network function                             |\n",
    "| critic_network_units           | (64, 64)   | Network topology for critic network function                            |\n",
    "| optimizer_learning_rate_actor  | 1e-3       | Initial learning rate for Adam optimizers used on actor networks        |\n",
    "| optimizer_learning_rate_critic | 1e-3       | Initial learning rate for Adam optimizers used on critic networks       |\n",
    "| optimizer_weight_decay_actor   | 0          | Weight decay for Adam optimizers used on actor networks                 |\n",
    "| optimizer_weight_decay_critic  | 0          | Weight decay for Adam optimizers used on critic networks                |\n",
    "| noise_scale                    | 0.1        | Scale for the Ornstein–Uhlenbeck noise process                          |\n",
    "| noise_theta                    | 0.2        | Theta parameter for the Ornstein–Uhlenbeck noise process                |\n",
    "| noise_sigma                    | 0.2        | Sigma parameter for the Ornstein–Uhlenbeck noise process                |\n",
    "| gamma                    | 0.99       | Discount rate for future rewards                                              |\n",
    "| tau                      | 1e-3       | Scaling parameter for soft target updates                                     |\n",
    "| batch_size               | 128        | Number of N-agent experiences to collect for a single optimization step       |\n",
    "| update_every             | 1          | Number of steps performed between updates                                     |\n",
    "| gradient_clip_actor      | 1.0        | Clipping parameter for gradient descent during actor optimization             |\n",
    "| gradient_clip_critic     | 1.0        | Clipping parameter for gradient descent during critic optimization            |\n",
    "\n",
    "Training hyperparameters are passed on the training function itself, `train_multiagent`, defined below.  The default values are:\n",
    "\n",
    "| parameter                     | value      | description                                           |\n",
    "|-------------------------------|------------|-------------------------------------------------------|\n",
    "| n_episodes                    | 4000       | Maximum number of training episodes                   |\n",
    "| max_t                         | 5000       | Maximum number of steps per episode                   |\n",
    "| solved_score                  | 1.5        | Average score over episodes required to consider problem solved     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "Import required system packages: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get the state and action dimensions, and initialize our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def train_multiagent(\n",
    "    env, \n",
    "    multiagent, \n",
    "    n_episodes=1000, \n",
    "    max_t=5000, \n",
    "    display_every=10,\n",
    "    solved_score=0.5, \n",
    "    save_filename=None\n",
    "):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):    \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        \n",
    "        n_actors = len(states)\n",
    "        score = np.zeros(n_actors)\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            actions = multiagent.act(states)\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = np.array(env_info.rewards)\n",
    "            dones = env_info.local_done\n",
    "                        \n",
    "            multiagent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            score += np.array(rewards)\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        max_score = score.max()               # get the maximum score over agents\n",
    "        scores_window.append(max_score)       # save most recent score\n",
    "        scores.append(max_score)              # save most recent score\n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage score: {:.4f}'.format(\n",
    "            i_episode, np.mean(scores_window)\n",
    "        ), end=\"\")\n",
    "        if i_episode % display_every == 0:\n",
    "            print('\\rEpisode {}\\tAverage score: {:.4f}'.format(\n",
    "                i_episode, np.mean(scores_window)\n",
    "            ))\n",
    "            \n",
    "        if np.mean(scores_window) >= solved_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage score: {:.4f}'.format(\n",
    "                np.maximum(i_episode-100, 0), np.mean(scores_window))\n",
    "             )\n",
    "            if save_filename is not None:\n",
    "                multiagent.save(save_filename)\n",
    "            break\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent\n",
    "\n",
    "from maddpg import MADDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage score: 0.0018\n",
      "Episode 200\tAverage score: 0.0000\n",
      "Episode 300\tAverage score: 0.0030\n",
      "Episode 400\tAverage score: 0.0000\n",
      "Episode 500\tAverage score: 0.0030\n",
      "Episode 600\tAverage score: 0.0110\n",
      "Episode 700\tAverage score: 0.0230\n",
      "Episode 800\tAverage score: 0.0280\n",
      "Episode 900\tAverage score: 0.0597\n",
      "Episode 1000\tAverage score: 0.0418\n",
      "Episode 1100\tAverage score: 0.0359\n",
      "Episode 1200\tAverage score: 0.0477\n",
      "Episode 1300\tAverage score: 0.0525\n",
      "Episode 1400\tAverage score: 0.1136\n",
      "Episode 1500\tAverage score: 0.3043\n",
      "Episode 1600\tAverage score: 0.7281\n",
      "Episode 1688\tAverage score: 1.5006\n",
      "Environment solved in 1588 episodes!\tAverage score: 1.5006\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "multiagent = MADDPG(\n",
    "    state_size=state_size, \n",
    "    action_size=action_size,\n",
    "    actor_network_units=(64, 64),\n",
    "    critic_network_units=(64, 64),\n",
    "    optimizer_learning_rate_actor=1e-3,\n",
    "    optimizer_learning_rate_critic=1e-3,\n",
    "    batch_size=128,\n",
    "    update_every=1,\n",
    "    noise_scale=0.1,\n",
    "    noise_theta=0.2,\n",
    "    noise_sigma=0.2\n",
    ")\n",
    "\n",
    "filename = 'checkpoint-tennis.pth'\n",
    "scores = train_multiagent(\n",
    "    env, \n",
    "    multiagent, \n",
    "    display_every=100, \n",
    "    n_episodes=10000,\n",
    "    max_t=5000,\n",
    "    solved_score=1.5, \n",
    "    save_filename=filename\n",
    ")\n",
    "multiagent.save(filename)\n",
    "\n",
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret / np.append(np.array(range(n)) + 1, n * np.ones(len(ret) - n))\n",
    "\n",
    "# plot the score and the average scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores, 'b')\n",
    "plt.plot(np.arange(len(moving_average(scores, n=100))), moving_average(scores, n=100), 'r')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 1.600000023841858\n"
     ]
    }
   ],
   "source": [
    "# Run through once with loaded model\n",
    "env_info = env.reset(train_mode=False)[brain_name]            # reset the environment    \n",
    "states = env_info.vector_observations                         # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                                 # initialize the score (for each agent)\n",
    "for t in range(1000):\n",
    "    actions = multiagent.act(states, noise=0)                 # get actions from model (for each agent)\n",
    "    env_info = env.step(actions)[brain_name]                  # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations                # get next state (for each agent)\n",
    "    rewards = env_info.rewards                                # get reward (for each agent)\n",
    "    dones = env_info.local_done                               # see if episode finished\n",
    "    scores += env_info.rewards                                # update the score (for each agent)\n",
    "    states = next_states                                      # roll over states to next time step\n",
    "    if np.any(dones):                                         # exit loop if episode finished\n",
    "        break\n",
    "\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, we can close the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future improvement\n",
    "\n",
    "- Use seeds for the random components in training in order to make the process reproducible\n",
    "- Associate separate memory buffers with each agent in order to allow ensemble training\n",
    "- Use Prioritized Experience Replay to select experiences to use in training each agent\n",
    "- Perform a more complete hyperparameter search\n",
    "- Try to solve the problem with other multi-agent algorithms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
